{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"VRE Charit\u00e9 Repositories Here are the README files from all the repositories in the vre-charite-autodeploy organization: helm-charts vre-infra frontend_vre_home service_entityinfo library_common service_scheduled_tasks service_dataops_gr service_dataops_utility core service_auth service_download service_pipelinewatch service_common service_dataset_neo4j service_encryption service_notification service_approval service_dataset service_provenance service_upload service_kg service_hpc service_cataloguing service_queue service_logger docs release-archive .github Additional Documentation Additional documentation files: vre-infra - DEPLOYMENT.md docs - vre-documentation.md","title":"Home"},{"location":"#vre-charite-repositories","text":"Here are the README files from all the repositories in the vre-charite-autodeploy organization: helm-charts vre-infra frontend_vre_home service_entityinfo library_common service_scheduled_tasks service_dataops_gr service_dataops_utility core service_auth service_download service_pipelinewatch service_common service_dataset_neo4j service_encryption service_notification service_approval service_dataset service_provenance service_upload service_kg service_hpc service_cataloguing service_queue service_logger docs release-archive .github","title":"VRE Charit\u00e9 Repositories"},{"location":"#additional-documentation","text":"Additional documentation files: vre-infra - DEPLOYMENT.md docs - vre-documentation.md","title":"Additional Documentation"},{"location":"core/","text":"core Core This is the git repository for Pilot project and the services used during its prototyping. Getting started Front End The folder /portal contains the front end project. Go inside this folder for details on how to install and run the VRE portal. HDFS The folder /hdfs contains the test Flask API used to connect to the HDFS service for file storage. Hive The folder /hive is for the Hive service. Syncope The folder /syncope contains the test Flask API used to connect to the Apache Syncope service for identity management. test Update readme for no real purpose","title":"core"},{"location":"core/#core","text":"","title":"core"},{"location":"core/#core_1","text":"This is the git repository for Pilot project and the services used during its prototyping.","title":"Core"},{"location":"core/#getting-started","text":"","title":"Getting started"},{"location":"core/#front-end","text":"The folder /portal contains the front end project. Go inside this folder for details on how to install and run the VRE portal.","title":"Front End"},{"location":"core/#hdfs","text":"The folder /hdfs contains the test Flask API used to connect to the HDFS service for file storage.","title":"HDFS"},{"location":"core/#hive","text":"The folder /hive is for the Hive service.","title":"Hive"},{"location":"core/#syncope","text":"The folder /syncope contains the test Flask API used to connect to the Apache Syncope service for identity management.","title":"Syncope"},{"location":"core/#test","text":"Update readme for no real purpose","title":"test"},{"location":"docs/","text":"docs Deploy MkDocs GitHub Pages This repository contains a GitHub Actions workflow that extracts README files from all repositories and provides them as documentation using mkdocs on GitHub Pages . How it works When new commits are pushed, the deploy pipeline in the deploy.yaml file is executed. The following list briefly describes what happens: Check out repository : The workflow starts by checking out the current repository. Set up Python : Python version 3.x is set up. Install dependencies : The required Python packages ( mkdocs , mkdocs-material , PyYAML , requests ) are installed. Fetch documentation : A Python script ( fetch_docs.py ) is executed to fetch the README files and additional documentation files from the repositories of the specified organization. Create MkDocs site : The documentation site is created using MkDocs. Deploy to GitHub Pages : The created documentation site is deployed to the gh-pages branch of the repository. Files fetch_docs.py : A Python script that fetches the README files and additional documentation files from the repositories of an organization and saves them in the docs directory. doc_map.yaml : An optional YAML file that defines additional documentation files for each repository. vre-documentation.md can serve as an example here. mkdocs.yml : The configuration file for MkDocs, which defines the structure and theme of the documentation site. Compile source This repo is primarily intended to be executed within a pipeline. It is not actually intended to be run manually. If desired, the following steps can be taken to obtain the compiled HTML files. Create and activate virtual environment (optional but recommended): : python -m venv venv source venv/bin/activate Install dependencies: : pip install mkdocs mkdocs-material PyYAML requests Fetch documentation: : Run the fetch_docs.py script to fetch the README files and additional documentation files from the repositories of the specified organization: python fetch_docs.py vre-charite-autodeploy Create MkDocs site : Create MkDocs site HTML pages: mkdocs build The created HTML pages can now be found in the build/ directory.","title":"docs"},{"location":"docs/#docs","text":"","title":"docs"},{"location":"docs/#deploy-mkdocs-github-pages","text":"This repository contains a GitHub Actions workflow that extracts README files from all repositories and provides them as documentation using mkdocs on GitHub Pages .","title":"Deploy MkDocs GitHub Pages"},{"location":"docs/#how-it-works","text":"When new commits are pushed, the deploy pipeline in the deploy.yaml file is executed. The following list briefly describes what happens: Check out repository : The workflow starts by checking out the current repository. Set up Python : Python version 3.x is set up. Install dependencies : The required Python packages ( mkdocs , mkdocs-material , PyYAML , requests ) are installed. Fetch documentation : A Python script ( fetch_docs.py ) is executed to fetch the README files and additional documentation files from the repositories of the specified organization. Create MkDocs site : The documentation site is created using MkDocs. Deploy to GitHub Pages : The created documentation site is deployed to the gh-pages branch of the repository.","title":"How it works"},{"location":"docs/#files","text":"fetch_docs.py : A Python script that fetches the README files and additional documentation files from the repositories of an organization and saves them in the docs directory. doc_map.yaml : An optional YAML file that defines additional documentation files for each repository. vre-documentation.md can serve as an example here. mkdocs.yml : The configuration file for MkDocs, which defines the structure and theme of the documentation site.","title":"Files"},{"location":"docs/#compile-source","text":"This repo is primarily intended to be executed within a pipeline. It is not actually intended to be run manually. If desired, the following steps can be taken to obtain the compiled HTML files. Create and activate virtual environment (optional but recommended): : python -m venv venv source venv/bin/activate Install dependencies: : pip install mkdocs mkdocs-material PyYAML requests Fetch documentation: : Run the fetch_docs.py script to fetch the README files and additional documentation files from the repositories of the specified organization: python fetch_docs.py vre-charite-autodeploy Create MkDocs site : Create MkDocs site HTML pages: mkdocs build The created HTML pages can now be found in the build/ directory.","title":"Compile source"},{"location":"docs_vre-documentation/","text":"docs - vre-documentation.md vre documentation self-hosting helm charts and images If you want to fork the vre project and host the helm charts and container images used for the deployment of the vre yourself, please note that several manual adjustments to the deployment code in the vre-infra repository . If you fork to another GitHub organization, simply search and replace all occurrences of vre-charite-autodeploy as well as vre-charite and replace them with the name of the organization you host your own helm charts and container images. If you changed names and/or versions, please make sure to adjust them accordingly as well. If you forked away from GitHub, search and replace all occurrences of ghcr.io/vre-charite-autodeploy as well as ghcr.io/vre-charite with the uris of your own helm and image registry, respectively. Either way, if your own registries are private, ensure the host that runs the deployment has the correct credentials available to access the helm registry. Similarly, within the kubernetes cluster, the correct image pull secrets must be configured to successfully fetch images from your private container registry.","title":"docs - vre-documentation.md"},{"location":"docs_vre-documentation/#docs-vre-documentationmd","text":"","title":"docs - vre-documentation.md"},{"location":"docs_vre-documentation/#vre-documentation","text":"","title":"vre documentation"},{"location":"docs_vre-documentation/#self-hosting-helm-charts-and-images","text":"If you want to fork the vre project and host the helm charts and container images used for the deployment of the vre yourself, please note that several manual adjustments to the deployment code in the vre-infra repository . If you fork to another GitHub organization, simply search and replace all occurrences of vre-charite-autodeploy as well as vre-charite and replace them with the name of the organization you host your own helm charts and container images. If you changed names and/or versions, please make sure to adjust them accordingly as well. If you forked away from GitHub, search and replace all occurrences of ghcr.io/vre-charite-autodeploy as well as ghcr.io/vre-charite with the uris of your own helm and image registry, respectively. Either way, if your own registries are private, ensure the host that runs the deployment has the correct credentials available to access the helm registry. Similarly, within the kubernetes cluster, the correct image pull secrets must be configured to successfully fetch images from your private container registry.","title":"self-hosting helm charts and images"},{"location":"frontend_vre_home/","text":"frontend_vre_home This is a Next.js project bootstrapped with create-next-app . Getting Started First, run the development server: npm run dev # or yarn dev Open http://localhost:3000 with your browser to see the result. You can start editing the page by modifying pages/index.tsx . The page auto-updates as you edit the file. API routes can be accessed on http://localhost:3000/api/hello . This endpoint can be edited in pages/api/hello.ts . The pages/api directory is mapped to /api/* . Files in this directory are treated as API routes instead of React pages. Learn More To learn more about Next.js, take a look at the following resources: Next.js Documentation - learn about Next.js features and API. Learn Next.js - an interactive Next.js tutorial. You can check out the Next.js GitHub repository - your feedback and contributions are welcome! Deploy on Vercel The easiest way to deploy your Next.js app is to use the Vercel Platform from the creators of Next.js. Check out our Next.js deployment documentation for more details. coding standards Folder - components - Common // for shared component - About - AboutHeader - AboutHeader.tsx - AboutHeader.module.scss - AboutFooter - AboutFooter.tsx - AboutFooter.module.scss - pages - index.tsx - about - index.tsx - index.module.scss - about-sub - index.tsx - index.module.scss for pages, all folder and .tsx files are kebab-case . For each route, defined a folder for that route, and create index.tsx for the page. directly use about/about-sub.tsx is not recommended, since we may introduce style file or test file for that .tsx in the future. Components are grouped by page in the compoents folder. For example, component/About contains all the components for about page. All the folders in component/About are flatten. That means each component has its own folder, and each folder only contains one component without nested folders. All folder and .tsx file in components are PascalCase . React Component import React, { useState } from \"react\"; // other imports // require or decomposing object const _ = require('lodash'); const { Content } = Layout; // local constant const MAX_ITEMS = 20; function About(props) { // 1. decompose props const {name,path,description} = props; // 2. hook for states const [loading, setLoading] = useState(false); const {username} = useSelector(state=>state); const func = useMemo(()=>{},[]); //3. useEffect useEffect(()=>{ func(); },[]); // 4. helper functions const submitForm = (form)=>{ // } // 5. event methods const onFinish = ()=>{ submitForm(form) } const onClick = ()=>{ }; // 6. data transformation for complicated data structure const birthplaces = users.map(user=>user.info?.birthplaces); // 7. if a component has too many props, extract them into a object and define it here const aboutSubProps = { name, path, loading, setLoading, //... }; // 8. if the returned jsx is too long, extract some of them into constant here const LeftSideBar = (<div>{/* around 10-50 lines of code */}</div>); // 9. return return <div> {LeftSideBar} <AboutSub {...aboutSubProps} /> </div> } The react component can be written as above. If the .tsx file is more than 250 lines, consider a refactoring. Scss file defined class as kebab-case , like about-header . Then use the class in jsx like: import styles from './index.module.scss' //... <div className={styles[\"about-header\"]} ></div>","title":"frontend_vre_home"},{"location":"frontend_vre_home/#frontend_vre_home","text":"This is a Next.js project bootstrapped with create-next-app .","title":"frontend_vre_home"},{"location":"frontend_vre_home/#getting-started","text":"First, run the development server: npm run dev # or yarn dev Open http://localhost:3000 with your browser to see the result. You can start editing the page by modifying pages/index.tsx . The page auto-updates as you edit the file. API routes can be accessed on http://localhost:3000/api/hello . This endpoint can be edited in pages/api/hello.ts . The pages/api directory is mapped to /api/* . Files in this directory are treated as API routes instead of React pages.","title":"Getting Started"},{"location":"frontend_vre_home/#learn-more","text":"To learn more about Next.js, take a look at the following resources: Next.js Documentation - learn about Next.js features and API. Learn Next.js - an interactive Next.js tutorial. You can check out the Next.js GitHub repository - your feedback and contributions are welcome!","title":"Learn More"},{"location":"frontend_vre_home/#deploy-on-vercel","text":"The easiest way to deploy your Next.js app is to use the Vercel Platform from the creators of Next.js. Check out our Next.js deployment documentation for more details.","title":"Deploy on Vercel"},{"location":"frontend_vre_home/#coding-standards","text":"","title":"coding standards"},{"location":"frontend_vre_home/#folder","text":"- components - Common // for shared component - About - AboutHeader - AboutHeader.tsx - AboutHeader.module.scss - AboutFooter - AboutFooter.tsx - AboutFooter.module.scss - pages - index.tsx - about - index.tsx - index.module.scss - about-sub - index.tsx - index.module.scss for pages, all folder and .tsx files are kebab-case . For each route, defined a folder for that route, and create index.tsx for the page. directly use about/about-sub.tsx is not recommended, since we may introduce style file or test file for that .tsx in the future. Components are grouped by page in the compoents folder. For example, component/About contains all the components for about page. All the folders in component/About are flatten. That means each component has its own folder, and each folder only contains one component without nested folders. All folder and .tsx file in components are PascalCase .","title":"Folder"},{"location":"frontend_vre_home/#react-component","text":"import React, { useState } from \"react\"; // other imports // require or decomposing object const _ = require('lodash'); const { Content } = Layout; // local constant const MAX_ITEMS = 20; function About(props) { // 1. decompose props const {name,path,description} = props; // 2. hook for states const [loading, setLoading] = useState(false); const {username} = useSelector(state=>state); const func = useMemo(()=>{},[]); //3. useEffect useEffect(()=>{ func(); },[]); // 4. helper functions const submitForm = (form)=>{ // } // 5. event methods const onFinish = ()=>{ submitForm(form) } const onClick = ()=>{ }; // 6. data transformation for complicated data structure const birthplaces = users.map(user=>user.info?.birthplaces); // 7. if a component has too many props, extract them into a object and define it here const aboutSubProps = { name, path, loading, setLoading, //... }; // 8. if the returned jsx is too long, extract some of them into constant here const LeftSideBar = (<div>{/* around 10-50 lines of code */}</div>); // 9. return return <div> {LeftSideBar} <AboutSub {...aboutSubProps} /> </div> } The react component can be written as above. If the .tsx file is more than 250 lines, consider a refactoring.","title":"React Component"},{"location":"frontend_vre_home/#scss-file","text":"defined class as kebab-case , like about-header . Then use the class in jsx like: import styles from './index.module.scss' //... <div className={styles[\"about-header\"]} ></div>","title":"Scss file"},{"location":"helm-charts/","text":"helm-charts helm-charts Helm Charts Repository for the Pilot Data Platform Usage Creating a new chart helm create mychart helm package mychart # will use version defined in chart mv mychart-x.y.z.tgz docs # move it to the github pages folder helm repo index docs --url https://pilotdataplatform.github.io/helm-charts/ # build index file for helm repository Using a chart from the git repo repo helm install deployment-name ./mychart Using a chart from the helm repository helm repo add pilot https://pilotdataplatform.github.io/helm-charts/ helm install deployment-name pilot/mychart","title":"helm-charts"},{"location":"helm-charts/#helm-charts","text":"","title":"helm-charts"},{"location":"helm-charts/#helm-charts_1","text":"Helm Charts Repository for the Pilot Data Platform","title":"helm-charts"},{"location":"helm-charts/#usage","text":"","title":"Usage"},{"location":"helm-charts/#creating-a-new-chart","text":"helm create mychart helm package mychart # will use version defined in chart mv mychart-x.y.z.tgz docs # move it to the github pages folder helm repo index docs --url https://pilotdataplatform.github.io/helm-charts/ # build index file for helm repository","title":"Creating a new chart"},{"location":"helm-charts/#using-a-chart-from-the-git-repo-repo","text":"helm install deployment-name ./mychart","title":"Using a chart from the git repo repo"},{"location":"helm-charts/#using-a-chart-from-the-helm-repository","text":"helm repo add pilot https://pilotdataplatform.github.io/helm-charts/ helm install deployment-name pilot/mychart","title":"Using a chart from the helm repository"},{"location":"library_common/","text":"library_common Common Library Installation Ensure you have Python 3.8 or later installed. Then, install the package using Poetry : poetry install","title":"library_common"},{"location":"library_common/#library_common","text":"","title":"library_common"},{"location":"library_common/#common-library","text":"","title":"Common Library"},{"location":"library_common/#installation","text":"Ensure you have Python 3.8 or later installed. Then, install the package using Poetry : poetry install","title":"Installation"},{"location":"release-archive/","text":"release-archive VRE Release Archive This repositories contains the archived sources of released versions of the VRE. The folder name denotes the git-tag set in VRE repositories and denotes the released version.","title":"release-archive"},{"location":"release-archive/#release-archive","text":"","title":"release-archive"},{"location":"release-archive/#vre-release-archive","text":"This repositories contains the archived sources of released versions of the VRE. The folder name denotes the git-tag set in VRE repositories and denotes the released version.","title":"VRE Release Archive"},{"location":"service_approval/","text":"service_approval service_approval","title":"service_approval"},{"location":"service_approval/#service_approval","text":"","title":"service_approval"},{"location":"service_approval/#service_approval_1","text":"","title":"service_approval"},{"location":"service_auth/","text":"service_auth This repo documents the service of user management. How to start the service sudo docker-compose up How to use the service login $ curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"testuser061501\",\"password\":\"!\", \"realm\":\"testrealms\"}' \\ http://127.0.0.1:5060/users/auth { \"result\": { \"access_token\": \"eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJrY3VRQjVCcDNTQlhRR0NpOVNCNDVjTGFnWnNRWVhkdi1OY3hQbzNJTU84In0.eyJleHAiOjE1OTM0NzQyNzAsImlhdCI6MTU5MzQ3MDY3MCwianRpIjoiYmMxMzNlNGMtNGQ2Ni00YjgxLTk4OWUtMTFkNmYzZTUyNDYzIiwiaXNzIjoiaHR0cDovLzEwLjMuOS4yNDE6ODA4MC9hdXRoL3JlYWxtcy90ZXN0cmVhbG1zIiwiYXVkIjoiYWNjb3VudCIsInN1YiI6IjI4NTI2ODY3LTAyY2MtNDlhZS1iNDA1LTE1NzA3ZmRiOGI0NSIsInR5cCI6IkJlYXJlciIsImF6cCI6ImtvbmciLCJzZXNzaW9uX3N0YXRlIjoiZDdmYWYxYTQtNzc1Ni00ZmY1LWJlMTYtMTliMWYxZDVhNGVmIiwiYWNyIjoiMSIsInJlYWxtX2FjY2VzcyI6eyJyb2xlcyI6WyJvZmZsaW5lX2FjY2VzcyIsInVtYV9hdXRob3JpemF0aW9uIl19LCJyZXNvdXJjZV9hY2Nlc3MiOnsiYWNjb3VudCI6eyJyb2xlcyI6WyJtYW5hZ2UtYWNjb3VudCIsIm1hbmFnZS1hY2NvdW50LWxpbmtzIiwidmlldy1wcm9maWxlIl19fSwic2NvcGUiOiJwcm9maWxlIGVtYWlsIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5hbWUiOiJ0ZXN0dXNlcjA2MTUwMSB0ZXN0dXNlcjA2MTUwMSIsInByZWZlcnJlZF91c2VybmFtZSI6InRlc3R1c2VyMDYxNTAxIiwiZ2l2ZW5fbmFtZSI6InRlc3R1c2VyMDYxNTAxIiwiZmFtaWx5X25hbWUiOiJ0ZXN0dXNlcjA2MTUwMSJ9.A0PI94CG_DksQfAWCYZMRNhzJ3N_gzpzlOUtnatVaq2HoLT5c3Y4sczZaFnXtOJpBB5KRTH1-RYZVhpPN_NWiyLUt_rwyWgk93AiKHWcCprdcz_PBCeYHPpgVaQuiadEMxTDrT1Cb8hB-E-QiH_bOMC-nWhQied7ZGWnR-fvEPZy08vv7dEr9xqtfkxYILg0SGccZKiBz7RmQBeOkU3W6ZBdkLGQvBL_anMrh-D1J2m_hebbDHZaoXKWIDVXlxF2CQ7Qk8FYCkHW2aSp5DWL97i2RdkyP7EnnrAC1IRCChnzylcHvnWOMIek51_iXi8KCXurlxhzk9MYVyVldjkZdQ\", \"expires_in\": 3600, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJjNTQ3ZTc1ZC01MWZjLTQwMDMtOGI3Yy1lYmJlMDFkODRmOTAifQ.eyJleHAiOjE1OTM0NzI0NzAsImlhdCI6MTU5MzQ3MDY3MCwianRpIjoiYWU5ZWEzMjItMDI4OS00NTliLWI2Y2MtN2I0ZTczMGVlNDk4IiwiaXNzIjoiaHR0cDovLzEwLjMuOS4yNDE6ODA4MC9hdXRoL3JlYWxtcy90ZXN0cmVhbG1zIiwiYXVkIjoiaHR0cDovLzEwLjMuOS4yNDE6ODA4MC9hdXRoL3JlYWxtcy90ZXN0cmVhbG1zIiwic3ViIjoiMjg1MjY4NjctMDJjYy00OWFlLWI0MDUtMTU3MDdmZGI4YjQ1IiwidHlwIjoiUmVmcmVzaCIsImF6cCI6ImtvbmciLCJzZXNzaW9uX3N0YXRlIjoiZDdmYWYxYTQtNzc1Ni00ZmY1LWJlMTYtMTliMWYxZDVhNGVmIiwic2NvcGUiOiJwcm9maWxlIGVtYWlsIn0.FJbpZBpdIfvAb0tuvGu1ToWIc4scKb0kK2t52AoB4Fw\", \"token_type\": \"bearer\", \"not-before-policy\": 0, \"session_state\": \"d7faf1a4-7756-4ff5-be16-19b1f1d5a4ef\", \"scope\": \"profile email\" } }","title":"service_auth"},{"location":"service_auth/#service_auth","text":"This repo documents the service of user management.","title":"service_auth"},{"location":"service_auth/#how-to-start-the-service","text":"sudo docker-compose up","title":"How to start the service"},{"location":"service_auth/#how-to-use-the-service","text":"login $ curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"testuser061501\",\"password\":\"!\", \"realm\":\"testrealms\"}' \\ http://127.0.0.1:5060/users/auth { \"result\": { \"access_token\": \"eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJrY3VRQjVCcDNTQlhRR0NpOVNCNDVjTGFnWnNRWVhkdi1OY3hQbzNJTU84In0.eyJleHAiOjE1OTM0NzQyNzAsImlhdCI6MTU5MzQ3MDY3MCwianRpIjoiYmMxMzNlNGMtNGQ2Ni00YjgxLTk4OWUtMTFkNmYzZTUyNDYzIiwiaXNzIjoiaHR0cDovLzEwLjMuOS4yNDE6ODA4MC9hdXRoL3JlYWxtcy90ZXN0cmVhbG1zIiwiYXVkIjoiYWNjb3VudCIsInN1YiI6IjI4NTI2ODY3LTAyY2MtNDlhZS1iNDA1LTE1NzA3ZmRiOGI0NSIsInR5cCI6IkJlYXJlciIsImF6cCI6ImtvbmciLCJzZXNzaW9uX3N0YXRlIjoiZDdmYWYxYTQtNzc1Ni00ZmY1LWJlMTYtMTliMWYxZDVhNGVmIiwiYWNyIjoiMSIsInJlYWxtX2FjY2VzcyI6eyJyb2xlcyI6WyJvZmZsaW5lX2FjY2VzcyIsInVtYV9hdXRob3JpemF0aW9uIl19LCJyZXNvdXJjZV9hY2Nlc3MiOnsiYWNjb3VudCI6eyJyb2xlcyI6WyJtYW5hZ2UtYWNjb3VudCIsIm1hbmFnZS1hY2NvdW50LWxpbmtzIiwidmlldy1wcm9maWxlIl19fSwic2NvcGUiOiJwcm9maWxlIGVtYWlsIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5hbWUiOiJ0ZXN0dXNlcjA2MTUwMSB0ZXN0dXNlcjA2MTUwMSIsInByZWZlcnJlZF91c2VybmFtZSI6InRlc3R1c2VyMDYxNTAxIiwiZ2l2ZW5fbmFtZSI6InRlc3R1c2VyMDYxNTAxIiwiZmFtaWx5X25hbWUiOiJ0ZXN0dXNlcjA2MTUwMSJ9.A0PI94CG_DksQfAWCYZMRNhzJ3N_gzpzlOUtnatVaq2HoLT5c3Y4sczZaFnXtOJpBB5KRTH1-RYZVhpPN_NWiyLUt_rwyWgk93AiKHWcCprdcz_PBCeYHPpgVaQuiadEMxTDrT1Cb8hB-E-QiH_bOMC-nWhQied7ZGWnR-fvEPZy08vv7dEr9xqtfkxYILg0SGccZKiBz7RmQBeOkU3W6ZBdkLGQvBL_anMrh-D1J2m_hebbDHZaoXKWIDVXlxF2CQ7Qk8FYCkHW2aSp5DWL97i2RdkyP7EnnrAC1IRCChnzylcHvnWOMIek51_iXi8KCXurlxhzk9MYVyVldjkZdQ\", \"expires_in\": 3600, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJjNTQ3ZTc1ZC01MWZjLTQwMDMtOGI3Yy1lYmJlMDFkODRmOTAifQ.eyJleHAiOjE1OTM0NzI0NzAsImlhdCI6MTU5MzQ3MDY3MCwianRpIjoiYWU5ZWEzMjItMDI4OS00NTliLWI2Y2MtN2I0ZTczMGVlNDk4IiwiaXNzIjoiaHR0cDovLzEwLjMuOS4yNDE6ODA4MC9hdXRoL3JlYWxtcy90ZXN0cmVhbG1zIiwiYXVkIjoiaHR0cDovLzEwLjMuOS4yNDE6ODA4MC9hdXRoL3JlYWxtcy90ZXN0cmVhbG1zIiwic3ViIjoiMjg1MjY4NjctMDJjYy00OWFlLWI0MDUtMTU3MDdmZGI4YjQ1IiwidHlwIjoiUmVmcmVzaCIsImF6cCI6ImtvbmciLCJzZXNzaW9uX3N0YXRlIjoiZDdmYWYxYTQtNzc1Ni00ZmY1LWJlMTYtMTliMWYxZDVhNGVmIiwic2NvcGUiOiJwcm9maWxlIGVtYWlsIn0.FJbpZBpdIfvAb0tuvGu1ToWIc4scKb0kK2t52AoB4Fw\", \"token_type\": \"bearer\", \"not-before-policy\": 0, \"session_state\": \"d7faf1a4-7756-4ff5-be16-19b1f1d5a4ef\", \"scope\": \"profile email\" } }","title":"How to use the service"},{"location":"service_cataloguing/","text":"service_cataloguing Metadata Service using Apache Atlas as metadata store to proxy the authorization The service will running at <host>:5064 Installation follow the step below to setup the service Clone Clone this repo to machine using https://git.indocresearch.org/platform/service_metadata.git Setup: To run the service as dev mode python3 -m pip install -r requirements.txt python3 app.py To run the service as production in docker and gunicorn docker build . -t metadata/latest docker run metadata/latest -d To add new entity in atlas run the curl in the type.txt it will add two more entity in atlas: nfs_file nfs_file_processed Features: the service uses the swagger to make the api documents: see the detailed doc Entity Related API: Add entity to atlas Query entity by the input payload Get entiy by the guid Audit Related API: Get audit of entity by guid","title":"service_cataloguing"},{"location":"service_cataloguing/#service_cataloguing","text":"","title":"service_cataloguing"},{"location":"service_cataloguing/#metadata-service","text":"using Apache Atlas as metadata store to proxy the authorization The service will running at <host>:5064","title":"Metadata Service"},{"location":"service_cataloguing/#installation","text":"follow the step below to setup the service","title":"Installation"},{"location":"service_cataloguing/#clone","text":"Clone this repo to machine using https://git.indocresearch.org/platform/service_metadata.git","title":"Clone"},{"location":"service_cataloguing/#setup","text":"To run the service as dev mode python3 -m pip install -r requirements.txt python3 app.py To run the service as production in docker and gunicorn docker build . -t metadata/latest docker run metadata/latest -d To add new entity in atlas run the curl in the type.txt it will add two more entity in atlas: nfs_file nfs_file_processed","title":"Setup:"},{"location":"service_cataloguing/#features","text":"the service uses the swagger to make the api documents: see the detailed doc","title":"Features:"},{"location":"service_cataloguing/#entity-related-api","text":"Add entity to atlas Query entity by the input payload Get entiy by the guid","title":"Entity Related API:"},{"location":"service_cataloguing/#audit-related-api","text":"Get audit of entity by guid","title":"Audit Related API:"},{"location":"service_common/","text":"service_common common Importable Pip package that generates entity ID and connects with Vault (secret engine) to retrieve credentials. Import package Pip install command from GitLab: pip install common --index-url https://__token__:<GITLAB_PAT>@git.indocresearch.org/api/v4/projects/158/packages/pypi/simple Pip install command from .whl file: pip install common-<VERSION>-py3-none-any.whl In requirements.txt : --index-url https://__token__:<GITLAB_PAT>@git.indocresearch.org/api/v4/projects/158/packages/pypi/simple common <GITLAB_PAT> is a GitLab personal access token with the read_api scope. Update package Refer to documentation: https://docs.gitlab.com/ee/user/packages/pypi_repository/#publish-a-pypi-package-by-using-twine Update the package version in setup.py . Otherwise, the push will fail with duplicate version number. Install the python build and twine package: python3 -m pip install --upgrade build python3 -m pip install --upgrade twine Build the common package: python3 -m build Now you will see two more folder generated under ./common/ |- common.egg-info/ |- dist/ Push to GitLab: TWINE_PASSWORD=<PASS> TWINE_USERNAME=<USER> python3 -m twine upload --repository-url https://git.indocresearch.org/api/v4/projects/158/packages/pypi dist/*","title":"service_common"},{"location":"service_common/#service_common","text":"","title":"service_common"},{"location":"service_common/#common","text":"Importable Pip package that generates entity ID and connects with Vault (secret engine) to retrieve credentials.","title":"common"},{"location":"service_common/#import-package","text":"Pip install command from GitLab: pip install common --index-url https://__token__:<GITLAB_PAT>@git.indocresearch.org/api/v4/projects/158/packages/pypi/simple Pip install command from .whl file: pip install common-<VERSION>-py3-none-any.whl In requirements.txt : --index-url https://__token__:<GITLAB_PAT>@git.indocresearch.org/api/v4/projects/158/packages/pypi/simple common <GITLAB_PAT> is a GitLab personal access token with the read_api scope.","title":"Import package"},{"location":"service_common/#update-package","text":"Refer to documentation: https://docs.gitlab.com/ee/user/packages/pypi_repository/#publish-a-pypi-package-by-using-twine Update the package version in setup.py . Otherwise, the push will fail with duplicate version number. Install the python build and twine package: python3 -m pip install --upgrade build python3 -m pip install --upgrade twine Build the common package: python3 -m build Now you will see two more folder generated under ./common/ |- common.egg-info/ |- dist/ Push to GitLab: TWINE_PASSWORD=<PASS> TWINE_USERNAME=<USER> python3 -m twine upload --repository-url https://git.indocresearch.org/api/v4/projects/158/packages/pypi dist/*","title":"Update package"},{"location":"service_dataops_gr/","text":"service_dataops_gr Data Operation Service Getting Started Prerequisites Python 3.7.3 You can find all prerequisites in requirements.txt , install all by pip install -r requirements.txt Start with python $ python app.py Start with gunicorn $ ./gunicorn_starter.sh Start with Docker, run on port 5063 $ sudo docker-compose up Notice that the service is depended on another service Neo4j Service . Folder Structure . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 access.log # gunicorn generated access log \u251c\u2500\u2500 app \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 app.py \u251c\u2500\u2500 config.py \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 error.log # gunicorn generated error log \u251c\u2500\u2500 gunicorn_config.py \u251c\u2500\u2500 gunicorn_starter.sh \u251c\u2500\u2500 nfs_ops # nfs operation apis \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 file_api.py \u2502 \u2514\u2500\u2500 folder_api.py \u251c\u2500\u2500 requirements.txt \u2514\u2500\u2500 resources \u2514\u2500\u2500 utils.py # helper functions Service All APIs will be documented in Dockerize data operation service","title":"service_dataops_gr"},{"location":"service_dataops_gr/#service_dataops_gr","text":"","title":"service_dataops_gr"},{"location":"service_dataops_gr/#data-operation-service","text":"","title":"Data Operation Service"},{"location":"service_dataops_gr/#getting-started","text":"Prerequisites Python 3.7.3 You can find all prerequisites in requirements.txt , install all by pip install -r requirements.txt Start with python $ python app.py Start with gunicorn $ ./gunicorn_starter.sh Start with Docker, run on port 5063 $ sudo docker-compose up Notice that the service is depended on another service Neo4j Service .","title":"Getting Started"},{"location":"service_dataops_gr/#folder-structure","text":". \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 access.log # gunicorn generated access log \u251c\u2500\u2500 app \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 app.py \u251c\u2500\u2500 config.py \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 error.log # gunicorn generated error log \u251c\u2500\u2500 gunicorn_config.py \u251c\u2500\u2500 gunicorn_starter.sh \u251c\u2500\u2500 nfs_ops # nfs operation apis \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 file_api.py \u2502 \u2514\u2500\u2500 folder_api.py \u251c\u2500\u2500 requirements.txt \u2514\u2500\u2500 resources \u2514\u2500\u2500 utils.py # helper functions","title":"Folder Structure"},{"location":"service_dataops_gr/#service","text":"All APIs will be documented in Dockerize data operation service","title":"Service"},{"location":"service_dataops_utility/","text":"service_dataops_utility DataOps Utility Service This service contains dataops that should not have access to greenroom. It's built using the FastAPI python framework. Prerequisites Poetry dependency manager. Installation Install Poetry . Configure access to internal package registry. poetry config http-basic.pilot ${PIP_USERNAME} ${PIP_PASSWORD} Install dependencies. poetry install Add environment variables into .env . Run application. poetry run python start.py Docker docker-compose docker-compose build docker-compose up Plain old docker docker build . -t service_data_ops docker run service_data_ops","title":"service_dataops_utility"},{"location":"service_dataops_utility/#service_dataops_utility","text":"","title":"service_dataops_utility"},{"location":"service_dataops_utility/#dataops-utility-service","text":"This service contains dataops that should not have access to greenroom. It's built using the FastAPI python framework.","title":"DataOps Utility Service"},{"location":"service_dataops_utility/#prerequisites","text":"Poetry dependency manager.","title":"Prerequisites"},{"location":"service_dataops_utility/#installation","text":"Install Poetry . Configure access to internal package registry. poetry config http-basic.pilot ${PIP_USERNAME} ${PIP_PASSWORD} Install dependencies. poetry install Add environment variables into .env . Run application. poetry run python start.py","title":"Installation"},{"location":"service_dataops_utility/#docker","text":"docker-compose docker-compose build docker-compose up Plain old docker docker build . -t service_data_ops docker run service_data_ops","title":"Docker"},{"location":"service_dataset/","text":"service_dataset dataset Dataset management","title":"service_dataset"},{"location":"service_dataset/#service_dataset","text":"","title":"service_dataset"},{"location":"service_dataset/#dataset","text":"Dataset management","title":"dataset"},{"location":"service_dataset_neo4j/","text":"service_dataset_neo4j Neo4j Api Service the service wrap up some basic neo4j query into API for further usage. The service will running at <host>:6062 Installation follow the step below to setup the service Clone Clone this repo to machine using https://git.indocresearch.org/platform/dataset_neo4j.git Prerequisites Poetry dependency manager. Vault connection credentials or custom-set environment variables. Installation Install Poetry . Configure access to internal package registry. poetry config http-basic.pilot ${PIP_USERNAME} ${PIP_PASSWORD} Install dependencies. poetry install Add environment variables into .env . Run application. poetry run python app.py Features: the service uses the swagger to make the api documents: see the detailed doc Node Related API: Retrieve node by id Retrieve node by relationship Query node by input payload Add new nodes Update nodes attributes Retrieve nodes beyond the relationship Relation Related API: Retrive the relation between nodes Add new relationship between nodes Update the relationship between nodes","title":"service_dataset_neo4j"},{"location":"service_dataset_neo4j/#service_dataset_neo4j","text":"","title":"service_dataset_neo4j"},{"location":"service_dataset_neo4j/#neo4j-api-service","text":"the service wrap up some basic neo4j query into API for further usage. The service will running at <host>:6062","title":"Neo4j Api Service"},{"location":"service_dataset_neo4j/#installation","text":"follow the step below to setup the service","title":"Installation"},{"location":"service_dataset_neo4j/#clone","text":"Clone this repo to machine using https://git.indocresearch.org/platform/dataset_neo4j.git","title":"Clone"},{"location":"service_dataset_neo4j/#prerequisites","text":"Poetry dependency manager. Vault connection credentials or custom-set environment variables.","title":"Prerequisites"},{"location":"service_dataset_neo4j/#installation_1","text":"Install Poetry . Configure access to internal package registry. poetry config http-basic.pilot ${PIP_USERNAME} ${PIP_PASSWORD} Install dependencies. poetry install Add environment variables into .env . Run application. poetry run python app.py","title":"Installation"},{"location":"service_dataset_neo4j/#features","text":"the service uses the swagger to make the api documents: see the detailed doc","title":"Features:"},{"location":"service_dataset_neo4j/#node-related-api","text":"Retrieve node by id Retrieve node by relationship Query node by input payload Add new nodes Update nodes attributes Retrieve nodes beyond the relationship","title":"Node Related API:"},{"location":"service_dataset_neo4j/#relation-related-api","text":"Retrive the relation between nodes Add new relationship between nodes Update the relationship between nodes","title":"Relation Related API:"},{"location":"service_download/","text":"service_download Download Service This service is built for file data downloading purpose. It's built using the FastAPI python framework. Installation Install requirements pip install -r requirements.txt Run the service with uvicorn python app.py Docker docker-compose docker-compose build docker-compose up","title":"service_download"},{"location":"service_download/#service_download","text":"","title":"service_download"},{"location":"service_download/#download-service","text":"This service is built for file data downloading purpose. It's built using the FastAPI python framework.","title":"Download Service"},{"location":"service_download/#installation","text":"","title":"Installation"},{"location":"service_download/#install-requirements","text":"pip install -r requirements.txt Run the service with uvicorn python app.py","title":"Install requirements"},{"location":"service_download/#docker","text":"docker-compose docker-compose build docker-compose up","title":"Docker"},{"location":"service_encryption/","text":"service_encryption service_encryption Development Requirements: - Python 3.7 - Virtualenv - Docker To run locally: virtualenv venv source venv/bin/activate pip install -r requirements python app.py To run with docker: docker build . -q docker run <image_id_from_build> Design Every file is encrypted with a random secret key generated at encryption/upload time. The communication of this key is securely transferred using both client and server public key exchange.","title":"service_encryption"},{"location":"service_encryption/#service_encryption","text":"","title":"service_encryption"},{"location":"service_encryption/#service_encryption_1","text":"","title":"service_encryption"},{"location":"service_encryption/#development","text":"Requirements: - Python 3.7 - Virtualenv - Docker To run locally: virtualenv venv source venv/bin/activate pip install -r requirements python app.py To run with docker: docker build . -q docker run <image_id_from_build>","title":"Development"},{"location":"service_encryption/#design","text":"Every file is encrypted with a random secret key generated at encryption/upload time. The communication of this key is securely transferred using both client and server public key exchange.","title":"Design"},{"location":"service_entityinfo/","text":"service_entityinfo Metadata Service This service is designed to create and update file metadata. Dependencies postgresql Quickstart Prepare environment and install dependencies poetry install","title":"service_entityinfo"},{"location":"service_entityinfo/#service_entityinfo","text":"","title":"service_entityinfo"},{"location":"service_entityinfo/#metadata-service","text":"This service is designed to create and update file metadata.","title":"Metadata Service"},{"location":"service_entityinfo/#dependencies","text":"postgresql","title":"Dependencies"},{"location":"service_entityinfo/#quickstart","text":"Prepare environment and install dependencies poetry install","title":"Quickstart"},{"location":"service_hpc/","text":"service_hpc service_hpc This service is built for communicating with HPC (auth, job submission, retrieve job info, retrieve node and partition info). It's built using the FastAPI python framework. Installation Install requirements pip install -r requirements.txt Run the service with uvicorn python3 run.py","title":"service_hpc"},{"location":"service_hpc/#service_hpc","text":"","title":"service_hpc"},{"location":"service_hpc/#service_hpc_1","text":"This service is built for communicating with HPC (auth, job submission, retrieve job info, retrieve node and partition info). It's built using the FastAPI python framework.","title":"service_hpc"},{"location":"service_hpc/#installation","text":"","title":"Installation"},{"location":"service_hpc/#install-requirements","text":"pip install -r requirements.txt Run the service with uvicorn python3 run.py","title":"Install requirements"},{"location":"service_kg/","text":"service_kg Knowledge Graph Knowledge Graph","title":"service_kg"},{"location":"service_kg/#service_kg","text":"","title":"service_kg"},{"location":"service_kg/#knowledge-graph","text":"Knowledge Graph","title":"Knowledge Graph"},{"location":"service_logger/","text":"service_logger Logger Package for configuring and writing logs. Quickstart Prepare environment and install dependencies poetry install Run tests poetry run pytest Build package poetry build Publish package poetry publish --repository indocresearch","title":"service_logger"},{"location":"service_logger/#service_logger","text":"","title":"service_logger"},{"location":"service_logger/#logger","text":"Package for configuring and writing logs.","title":"Logger"},{"location":"service_logger/#quickstart","text":"Prepare environment and install dependencies poetry install Run tests poetry run pytest Build package poetry build Publish package poetry publish --repository indocresearch","title":"Quickstart"},{"location":"service_notification/","text":"service_notification service_notification About Manages emails and system maintenance notifications. Built With Python FastAPI Getting Started Prerequisites Poetry dependency manager. Vault connection credentials or custom-set environment variables. Installation Using Docker Run Docker compose with environment variables. PIP_USERNAME=[...] PIP_PASSWORD=[...] docker-compose up Find service locally at http://localhost:5065/ . Without Docker Install Poetry . Configure access to internal package registry. poetry config http-basic.pilot ${PIP_USERNAME} ${PIP_PASSWORD} Install dependencies. poetry install Add environment variables into .env . Run application. poetry run python run.py Find service locally at http://localhost:5065/ . Example: poetry install poetry run python run.py CONFIG_CENTER_ENABLED=true VAULT_URL=[...] VAULT_CRT=[...] VAULT_TOKEN=[...] poetry run python run.py Usage Swagger API documentation can be found locally at http://localhost:5065/v1/api-doc .","title":"service_notification"},{"location":"service_notification/#service_notification","text":"","title":"service_notification"},{"location":"service_notification/#service_notification_1","text":"","title":"service_notification"},{"location":"service_notification/#about","text":"Manages emails and system maintenance notifications.","title":"About"},{"location":"service_notification/#built-with","text":"Python FastAPI","title":"Built With"},{"location":"service_notification/#getting-started","text":"","title":"Getting Started"},{"location":"service_notification/#prerequisites","text":"Poetry dependency manager. Vault connection credentials or custom-set environment variables.","title":"Prerequisites"},{"location":"service_notification/#installation","text":"","title":"Installation"},{"location":"service_notification/#using-docker","text":"Run Docker compose with environment variables. PIP_USERNAME=[...] PIP_PASSWORD=[...] docker-compose up Find service locally at http://localhost:5065/ .","title":"Using Docker"},{"location":"service_notification/#without-docker","text":"Install Poetry . Configure access to internal package registry. poetry config http-basic.pilot ${PIP_USERNAME} ${PIP_PASSWORD} Install dependencies. poetry install Add environment variables into .env . Run application. poetry run python run.py Find service locally at http://localhost:5065/ . Example: poetry install poetry run python run.py CONFIG_CENTER_ENABLED=true VAULT_URL=[...] VAULT_CRT=[...] VAULT_TOKEN=[...] poetry run python run.py","title":"Without Docker"},{"location":"service_notification/#usage","text":"Swagger API documentation can be found locally at http://localhost:5065/v1/api-doc .","title":"Usage"},{"location":"service_pipelinewatch/","text":"service_pipelinewatch service_pipelinewatch Pipelinewatch is a watcher program that watches the status and progress of data pipeline jobs, based on k8s jobs.","title":"service_pipelinewatch"},{"location":"service_pipelinewatch/#service_pipelinewatch","text":"","title":"service_pipelinewatch"},{"location":"service_pipelinewatch/#service_pipelinewatch_1","text":"Pipelinewatch is a watcher program that watches the status and progress of data pipeline jobs, based on k8s jobs.","title":"service_pipelinewatch"},{"location":"service_provenance/","text":"service_provenance service_provenance This service is using the FastAPI python framework. Installation Install requirements pip install -r requirements.txt Run the service with uvicorn python run.py Docker docker-compose docker-compose build docker-compose up Unit Test *directly run command below pytest","title":"service_provenance"},{"location":"service_provenance/#service_provenance","text":"","title":"service_provenance"},{"location":"service_provenance/#service_provenance_1","text":"This service is using the FastAPI python framework.","title":"service_provenance"},{"location":"service_provenance/#installation","text":"","title":"Installation"},{"location":"service_provenance/#install-requirements","text":"pip install -r requirements.txt Run the service with uvicorn python run.py","title":"Install requirements"},{"location":"service_provenance/#docker","text":"docker-compose docker-compose build docker-compose up","title":"Docker"},{"location":"service_provenance/#unit-test","text":"*directly run command below pytest","title":"Unit Test"},{"location":"service_queue/","text":"service_queue README.md not found.","title":"service_queue"},{"location":"service_queue/#service_queue","text":"README.md not found.","title":"service_queue"},{"location":"service_scheduled_tasks/","text":"service_scheduled_tasks README.md not found.","title":"service_scheduled_tasks"},{"location":"service_scheduled_tasks/#service_scheduled_tasks","text":"README.md not found.","title":"service_scheduled_tasks"},{"location":"service_upload/","text":"service_upload Upload Service This service is built for file data uploading purpose. It's built using the FastAPI python framework. About The Project The upload service is one of the component for PILOT project. The main responsibility is to handle the file upload(especially large file). The main machanism for uploading is to chunk up the large file(>2MB). It has three main api for pre-uploading, uploading chunks and combining the chunks. After combining the chunks, the api will upload the file to Minio as the Object Storage. Built With Minio : The Object Storage to save the data Fastapi : The async api framework for backend poetry : python package management Getting Started Prerequisites The project is using poetry to handle the package. Note here the poetry must install globally not in the anaconda virtual environment pip install poetry create the .env file from .env.schema Installation git clone the project: git clone https://github.com/PilotDataPlatform/upload.git install the package: poetry install run it locally: poetry run python run.py Docker To package up the service into docker pod, running following command: docker build --build-arg pip_username=<pip_username> --build-arg pip_password=<pip_password> API Documents REST API documentation in the form of Swagger/OpenAPI can be found here: Api Document Helm Charts Components of the Pilot Platform are available as helm charts for installation on Kubernetes: Upload Service Helm Charts","title":"service_upload"},{"location":"service_upload/#service_upload","text":"","title":"service_upload"},{"location":"service_upload/#upload-service","text":"This service is built for file data uploading purpose. It's built using the FastAPI python framework.","title":"Upload Service"},{"location":"service_upload/#about-the-project","text":"The upload service is one of the component for PILOT project. The main responsibility is to handle the file upload(especially large file). The main machanism for uploading is to chunk up the large file(>2MB). It has three main api for pre-uploading, uploading chunks and combining the chunks. After combining the chunks, the api will upload the file to Minio as the Object Storage.","title":"About The Project"},{"location":"service_upload/#built-with","text":"Minio : The Object Storage to save the data Fastapi : The async api framework for backend poetry : python package management","title":"Built With"},{"location":"service_upload/#getting-started","text":"","title":"Getting Started"},{"location":"service_upload/#prerequisites","text":"The project is using poetry to handle the package. Note here the poetry must install globally not in the anaconda virtual environment pip install poetry create the .env file from .env.schema","title":"Prerequisites"},{"location":"service_upload/#installation","text":"git clone the project: git clone https://github.com/PilotDataPlatform/upload.git install the package: poetry install run it locally: poetry run python run.py","title":"Installation"},{"location":"service_upload/#docker","text":"To package up the service into docker pod, running following command: docker build --build-arg pip_username=<pip_username> --build-arg pip_password=<pip_password>","title":"Docker"},{"location":"service_upload/#api-documents","text":"REST API documentation in the form of Swagger/OpenAPI can be found here: Api Document","title":"API Documents"},{"location":"service_upload/#helm-charts","text":"Components of the Pilot Platform are available as helm charts for installation on Kubernetes: Upload Service Helm Charts","title":"Helm Charts"},{"location":"vre-infra/","text":"vre-infra vre-infra Charite VRE Deployment repo for helm charts, config values, etc... current workflow: be sure to have the right kubeconfig loaded, default is ~/.kube/config test your connection to the cluster kubectl get pods -A deploy into the cluster DEPLOYMENT-GUIDE to destoy current deployment switch into the terraform folder use terraform for deployment, see terraform documentation on how to this, basically it's terraform destroy Coding Style This is basically a terraform repository. There are certain things such as formatting which terraform natively takes care of. Please run terraform validate and terraform fmt -recursive before commiting code. Currently, we don't do pipeline checks, but it's very likely we will add this in the future. We don't do linting at this point, so please try to follow these official recommendations . Use underscores to separate multiple words in (resource) names. Vault added namespace for vault added helm_release for vault added values.yaml for fault disabled developer mode; enabled tls [!Tip] To unseal the vault after initial setup, connect to shell of the vault pod. Run vault operator init and save the listed unseal keys and the initial root token. To unseal the vault run vault operator unseal at least three times. Each time present another unseal key. You may need to append -tls-skip-verify after each command, since we are using a self-signed certificate. PKI - Cert-Manager et al. We use a self-signed PKI to issue server certificates for the applications deployed to VRE (as of now). In the following, we roughly depict the dependencies among the involved components and explain the consequences for the deployment order. As we rely on cert-manager for the certificate distribution and renewal, we have to install cert-manager first in the cluster. Basically, in order to make cert-manager's custom resource definition (crd)s available in the cluster as those need to be present in the cluster for subsequent terraform plan steps to succeed that want to generate certificate resources from those crds. We use trust-manager to make the root certificate available to applications in the cluster such that those applications can verify the presented server certificate up to the root. All our server certificates are immediately issued from the root for the sake of simplicity. Thus, client applications need to know / trust the root certificate otherwise the tls connection attempt to the server fails. Minio is such a candidate that needs that kind of trust establishment, for instance. Operator and tenant communicate and the operator needs to trust the server certificate presented by the tenant. Furthermore, minio's tenant and operator deployment require their respective server certificates to be present already in the cluster at deployment time. Other applications thus far follow standard procedures pretty much and are thus not discussed in detail. The source code should suffice as documentation. Deployment order Hence, cert-manager et al. have to be rolled-out in the following order: pre-installation : cert-manager and trust-manager intermediary-installation : the vre root certificate and the cluster issuer (issues new certificates derived from the root) minio server certificates for both tenant and operator installation: : minio tenant and operator all other applications that rely on the pki to be fully installed in the cluster","title":"vre-infra"},{"location":"vre-infra/#vre-infra","text":"","title":"vre-infra"},{"location":"vre-infra/#vre-infra_1","text":"Charite VRE Deployment repo for helm charts, config values, etc...","title":"vre-infra"},{"location":"vre-infra/#current-workflow","text":"be sure to have the right kubeconfig loaded, default is ~/.kube/config test your connection to the cluster kubectl get pods -A","title":"current workflow:"},{"location":"vre-infra/#deploy-into-the-cluster","text":"DEPLOYMENT-GUIDE","title":"deploy into the cluster"},{"location":"vre-infra/#to-destoy-current-deployment","text":"switch into the terraform folder use terraform for deployment, see terraform documentation on how to this, basically it's terraform destroy","title":"to destoy current deployment"},{"location":"vre-infra/#coding-style","text":"This is basically a terraform repository. There are certain things such as formatting which terraform natively takes care of. Please run terraform validate and terraform fmt -recursive before commiting code. Currently, we don't do pipeline checks, but it's very likely we will add this in the future. We don't do linting at this point, so please try to follow these official recommendations . Use underscores to separate multiple words in (resource) names.","title":"Coding Style"},{"location":"vre-infra/#vault","text":"added namespace for vault added helm_release for vault added values.yaml for fault disabled developer mode; enabled tls [!Tip] To unseal the vault after initial setup, connect to shell of the vault pod. Run vault operator init and save the listed unseal keys and the initial root token. To unseal the vault run vault operator unseal at least three times. Each time present another unseal key. You may need to append -tls-skip-verify after each command, since we are using a self-signed certificate.","title":"Vault"},{"location":"vre-infra/#pki-cert-manager-et-al","text":"We use a self-signed PKI to issue server certificates for the applications deployed to VRE (as of now). In the following, we roughly depict the dependencies among the involved components and explain the consequences for the deployment order. As we rely on cert-manager for the certificate distribution and renewal, we have to install cert-manager first in the cluster. Basically, in order to make cert-manager's custom resource definition (crd)s available in the cluster as those need to be present in the cluster for subsequent terraform plan steps to succeed that want to generate certificate resources from those crds. We use trust-manager to make the root certificate available to applications in the cluster such that those applications can verify the presented server certificate up to the root. All our server certificates are immediately issued from the root for the sake of simplicity. Thus, client applications need to know / trust the root certificate otherwise the tls connection attempt to the server fails. Minio is such a candidate that needs that kind of trust establishment, for instance. Operator and tenant communicate and the operator needs to trust the server certificate presented by the tenant. Furthermore, minio's tenant and operator deployment require their respective server certificates to be present already in the cluster at deployment time. Other applications thus far follow standard procedures pretty much and are thus not discussed in detail. The source code should suffice as documentation.","title":"PKI - Cert-Manager et al."},{"location":"vre-infra/#deployment-order","text":"Hence, cert-manager et al. have to be rolled-out in the following order: pre-installation : cert-manager and trust-manager intermediary-installation : the vre root certificate and the cluster issuer (issues new certificates derived from the root) minio server certificates for both tenant and operator installation: : minio tenant and operator all other applications that rely on the pki to be fully installed in the cluster","title":"Deployment order"},{"location":"vre-infra_DEPLOYMENT/","text":"vre-infra - DEPLOYMENT.md VRE DEPLOYMENT REQUIREMENTS [ ] A Kubernetes cluster [ ] Local tools installed for deploying [ ] Valid Github Personal Access Token (PAT) Kubernetes * [ ] Any kubernetes cluster (vanilla or from a distribution, < = 1.29.X) + accessible via kubeconfig from the machine you want deploy vre from * [ ] Storageprovider/class (RMO/RWX) is already configured on the cluster (PVC can be created / will be bound) * [ ] Services from LoadBalancer can be created (e.g. Cloud Provider based, HW, CiliumLB or MetalLB is installed on cluster and/or configured) Tools | Tool | Example Version | Download Link | Used For | |------------|----------------|----------------------------------------------------------------------|--------------------------------------------------------------------------| | Terraform | v1.10.5 | [Terraform Downloads](https://www.terraform.io/downloads.html) | Automates the provisioning and configuration of VRE. | | Helm | v3.17.0 | [Helm Install](https://helm.sh/docs/intro/install/) | Verify Helm Releases. | | Kubectl | v1.32.3 | [Kubectl Downloads](https://kubernetes.io/docs/tasks/tools/) | Provides command-line control over the Kubernetes clusters. | | k9s | v0.40.5 | [k9s Releases](https://github.com/derailed/k9s/releases) | Offers a terminal UI to monitor and troubleshoot Kubernetes resources. | Token To create a Personal Access Token on GitHub with the required permissions: * Log in to your GitHub account. * Click your profile picture in the top-right corner and select Settings. * In the left sidebar, scroll down to Developer settings. * Choose Personal access tokens, then: * click Tokens (classic) and then Generate new token. * After that, select the permissions you need (such as repo, workflow, write:packages, etc.), and then generate the token. Copy and store it securely. Repository Permissions (repo) \u2705 repo (Full control of private repositories) \u2705 repo:status (Access commit status) \u2705 repo_deployment (Access deployment status) \u2705 public_repo (Access public repositories) \u2705 repo:invite (Access repository invitations) \u2705 security_events (Read and write security events) \u2705 read:repo_hook (Read repository hooks) Workflow Permissions \u2705 workflow (Update GitHub Action workflows) Packages Permissions \u2705 write:packages (Upload packages to GitHub Package Registry) \u2705 read:packages (Download packages from GitHub Package Registry) Organization Permissions \u2705 manage_runners:org (Manage org runners and runner groups) PREPARATION CLONE REPO git clone https://github.com/vre-charite-dev/vre-infra.git cd vre-infra OPTIONAL: CLONE BY TAG # ONCE / IF THERE ARE TAGS YOU COULD SWITCH THAT WAY TO A RELEASED VERSION git checkout tags/<tag-name> OPTIONAL: CHECKOUT A SPECIFC HASH FROM COMMIT HISTORY git checkout <commit-hash> CONFIGURATION OPTION A: EDIT EXISTING CONFIGURATION FILE # EDIT FILE / MAKE CHANGES e.g vi ./terraform/config/charite/charite.tfvars OPTION B: CREATE NEW ENVIORONMENT / CONFIGURATION FILE ENV=production # just an example name PATH_DEFAULT_CONFIG=./terraform/config/charite/charite.tfvars PATH_NEW_CONFIG=./terraform/config/${ENV}/${ENV}.tfvars # CREATE FOLDER FOR ENV mkdir -p ./terraform/config/${ENV}/ cp ${PATH_DEFAULT_CONFIG} ${PATH_NEW_CONFIG} # EDIT FILE / MAKE CHANGES e.g. vi ${PATH_NEW_CONFIG} SECRETS OPTION A: ADD SECRETS TO ENV export TF_VAR_ghcr_token=\"${GITHUB_USERNAME}:${GITHUB_TOKEN}\" export TF_VAR_vault_token=\"\" export TF_VAR_kubeconfig=/path/to/kube/config OPTION B: ADD NEEDED SECRETS TO A FILE (SECRETS.AUTO.TFVARS) # CREATE THE SECRETS.AUTO.TFVARS FILE IN THE TERRAFORM FOLDERS GITHUB_USERNAME=\"<CHANGE-ME>\" GITHUB_TOKEN=\"<CHANGE-ME>\" VAULT_TOKEN=\"<CHANGE-ME>\" FOLDERS=(\"pre-install\" \"install\" \"intermediary-install\" \"post-install\") for TF_FOLDER in \"${FOLDERS[@]}\"; do cat <<EOF > \"./terraform/${TF_FOLDER}/secrets.auto.tfvars\" ghcr_token = \"${GITHUB_USERNAME}:${GITHUB_TOKEN}\" vault_token = \"${VAULT_TOKEN}\" EOF done DEPLOYMENT OPTION A: APPLY VRE - ALL AT ONCE PATH_CONFIG=./terraform/config/charite/charite.tfvars # or change to a newly created env config file FOLDERS=(\"pre-install\" \"install\" \"intermediary-install\" \"post-install\") VAR_FILE=${PATH_CONFIG} for TF_FOLDER in \"${FOLDERS[@]}\"; do terraform -chdir=./terraform/${TF_FOLDER} init terraform -chdir=./terraform/${TF_FOLDER} apply --auto-approve -var-file=${VAR_FILE} done OPTION B: APPLY VRE - ONE BY ONE ### 1. PRE-INSTALL pre-install creates: * cert-manager, ingress, observability, etc. * multiple secrets for authentication (regcred and vault-secret across various namespaces) * two Helm releases (cert-manager and trust-manager) Apply pre-install cd ./terraform/pre-install terraform init terraform apply --auto-approve Verify pre-install w/ kubectl+helm export KUBECONFIG=/path/to/kube/config # GENERAL kubectl get namespaces # EXAMPLE CHECKS kubectl get namespace cert-manager ingress observability kubectl get secret regcred -n cert-manager kubectl describe secret regcred -n cert-manager # GENERAL helm list -A # EXAMPLE CHECKS helm status cert-manager -n cert-manager helm status trust-manager -n cert-manager kubectl get pods -n cert-manager kubectl get pods -n ingress kubectl get pods -n observability ### 2. INTERMEDIARY-INSTALL intermediary-install creates: * cert-manager resources, including a self-signed issuer, a root certificate + cluster issuer * server certificates were generated for the operator and a MinIO tenant * jaeger operator Apply intermediary-install cd ./terraform/intermediary-install # or cd ../intermediary-install changing from previous apply operation terraform init terraform apply --auto-approve Verify intermediary-install w/ kubectl+helm # List all ClusterIssuers and Issuers kubectl get clusterissuers,issuers -A # Describe a specific ClusterIssuer kubectl describe clusterissuer <issuer-name> # List all Certificates kubectl get certificates -A # Describe a specific Certificate kubectl describe certificate <certificate-name> -n <namespace> # Check CertificateRequest status kubectl get certificaterequests -A # List Secrets to confirm certificates were created kubectl get secrets -A | grep tls # Inspect a specific TLS secret kubectl describe secret <secret-name> -n <namespace> # Check Helm releases helm list -A | grep jaeger # Get detailed status of Jaeger Operator Helm release helm status jaeger -n <namespace> # Check Jaeger Operator pods kubectl get pods -n <namespace> | grep jaeger # Describe Jaeger Operator deployment kubectl describe deployment jaeger-operator -n <namespace> # Check logs for troubleshooting kubectl logs -l app.kubernetes.io/name=jaeger-operator -n <namespace> ### 3. INSTALL install creates: * Core Services: Kong, Redis, Elasticsearch, Neo4j, OpenLDAP, MinIO. * Application Services: Approval, Encryption, Cataloguing, Dataset, Provenance, Notification, etc. * Utility Services: Mailhog, Queue Consumer, Queue Producer, Pipeline Watch, etc. Apply install cd ./terraform/install # or cd ../install changing from previous apply operation terraform init terraform apply --auto-approve -var-file=\"../config/charite/charite.tfvars\" Verify install w/ kubectl+helm # List All Helm Releases helm list --all-namespaces # Check Resources for a Specific Helm Release helm status <release-name> -n <namespace> # Example Commands for Specific Releases helm status kong -n <namespace> kubectl get pods -n <namespace> -l app.kubernetes.io/instance=kong helm status redis -n <namespace> kubectl get services -n <namespace> -l app.kubernetes.io/instance=redis kubectl get configmaps -n <namespace> -l app.kubernetes.io/instance=<release-name> kubectl get secrets -n <namespace> -l app.kubernetes.io/instance=<release-name> kubectl get pv kubectl get pvc -n <namespace> ### 4. POST-INSTALL post-install creates: * Atlas: ConfigMap and Job for Atlas configuration. * Elasticsearch and Kong: ConfigMaps and Jobs for Elasticsearch and Kong setup. * Keycloak: * A new realm (vre). * Multiple OpenID clients (react_app_client, minio_client, kong_client). * Roles (platform_admin, admin_role). * Protocol mappers for client configurations. * A user (admin_user) with assigned roles. * Service account roles and client default scopes. Apply post-install CONFIG_FILE=\"../config/charite/charite.tfvars\" # example cd ./terraform/post-install # or cd ../post-install changing from previous apply operation terraform init terraform apply --auto-approve -var-file=\"${CONFIG_FILE} Verify post-install w/ kubectl+helm # Check Atlas ConfigMap kubectl get configmap atlas-custom-entity -n utility # Check Elasticsearch ConfigMap kubectl get configmap elasticsearch-index-configmap -n <namespace> # Check Elasticsearch Job kubectl get job elasticsearch-index-job -n <namespace> # Check Kong ConfigMap kubectl get configmap kong-configmap -n <namespace> # Check Kong Job kubectl get job kong-job -n <namespace> # Check Atlas Job kubectl get job atlas-config-job -n utility # Verify Keycloak Realm (vre) kubectl exec -it <keycloak-pod> -n <namespace> -- /opt/keycloak/bin/kcadm.sh get realms/vre # Verify OpenID Clients kubectl exec -it <keycloak-pod> -n <namespace> -- /opt/keycloak/bin/kcadm.sh get clients -r vre # Verify Client Default Scopes kubectl exec -it <keycloak-pod> -n <namespace> -- /opt/keycloak/bin/kcadm.sh get clients/<client-id>/default-client-scopes -r vre TROUBLESHOOTING TERRAFORM Check Terraform Output Terraform usually provides error messages when a run fails. Review the output: terraform apply --auto-approve To capture the output into a file for easier review: terraform apply --auto-approve | tee terraform_output.log Verify Environment Variables echo $GITHUB_USERNAME echo $GITHUB_TOKEN | sed 's/./*/g' # Masked output for security echo $VAULT_TOKEN | sed 's/./*/g' Alternatively, use a .env file and source it: source .env Validate Terraform Configuration Check for syntax or structural errors in the Terraform files: terraform validate If validation passes but apply still fails, check the execution plan: terraform plan Enable Debug Logging For more details on the error, enable debug logging: export TF_LOG=DEBUG terraform apply --auto-approve 2>&1 | tee terraform_debug.log Restart Terraform from Scratch If troubleshooting doesn\u2019t resolve the issue, reset Terraform and try again: terraform destroy --auto-approve # Destroy existing resources rm -rf .terraform/ terraform.tfstate* # Delete local state terraform init terraform apply --auto-approve UNINSTALL OPTION A: DESTROY VRE - ALL AT ONCE PATH_CONFIG=$(pwd)/terraform/config/charite/charite.tfvars # or change to a newly created env config file FOLDERS=(\"post-install\" \"install\" \"intermediary-install\" \"pre-install\") VAR_FILE=${PATH_CONFIG} for TF_FOLDER in \"${FOLDERS[@]}\"; do echo destroying ${TF_FOLDER} terraform -chdir=./terraform/${TF_FOLDER} destroy --auto-approve -var-file=${VAR_FILE} echo destroy ${TF_FOLDER} complete! done OPTION B: DESTROY SPECIFIC VRE DEPLOYMENT STEP CONFIG_FILE=\"../config/charite/charite.tfvars\" # example, needed for at least install & post-install steps cd ./terraform/<INSTALL-NAME> # e.g. install or post-install terraform destroy --auto-approve -var-file=\"${CONFIG_FILE}","title":"vre-infra - DEPLOYMENT.md"},{"location":"vre-infra_DEPLOYMENT/#vre-infra-deploymentmd","text":"","title":"vre-infra - DEPLOYMENT.md"},{"location":"vre-infra_DEPLOYMENT/#vre-deployment","text":"","title":"VRE DEPLOYMENT"},{"location":"vre-infra_DEPLOYMENT/#requirements","text":"[ ] A Kubernetes cluster [ ] Local tools installed for deploying [ ] Valid Github Personal Access Token (PAT) Kubernetes * [ ] Any kubernetes cluster (vanilla or from a distribution, < = 1.29.X) + accessible via kubeconfig from the machine you want deploy vre from * [ ] Storageprovider/class (RMO/RWX) is already configured on the cluster (PVC can be created / will be bound) * [ ] Services from LoadBalancer can be created (e.g. Cloud Provider based, HW, CiliumLB or MetalLB is installed on cluster and/or configured) Tools | Tool | Example Version | Download Link | Used For | |------------|----------------|----------------------------------------------------------------------|--------------------------------------------------------------------------| | Terraform | v1.10.5 | [Terraform Downloads](https://www.terraform.io/downloads.html) | Automates the provisioning and configuration of VRE. | | Helm | v3.17.0 | [Helm Install](https://helm.sh/docs/intro/install/) | Verify Helm Releases. | | Kubectl | v1.32.3 | [Kubectl Downloads](https://kubernetes.io/docs/tasks/tools/) | Provides command-line control over the Kubernetes clusters. | | k9s | v0.40.5 | [k9s Releases](https://github.com/derailed/k9s/releases) | Offers a terminal UI to monitor and troubleshoot Kubernetes resources. | Token To create a Personal Access Token on GitHub with the required permissions: * Log in to your GitHub account. * Click your profile picture in the top-right corner and select Settings. * In the left sidebar, scroll down to Developer settings. * Choose Personal access tokens, then: * click Tokens (classic) and then Generate new token. * After that, select the permissions you need (such as repo, workflow, write:packages, etc.), and then generate the token. Copy and store it securely. Repository Permissions (repo) \u2705 repo (Full control of private repositories) \u2705 repo:status (Access commit status) \u2705 repo_deployment (Access deployment status) \u2705 public_repo (Access public repositories) \u2705 repo:invite (Access repository invitations) \u2705 security_events (Read and write security events) \u2705 read:repo_hook (Read repository hooks) Workflow Permissions \u2705 workflow (Update GitHub Action workflows) Packages Permissions \u2705 write:packages (Upload packages to GitHub Package Registry) \u2705 read:packages (Download packages from GitHub Package Registry) Organization Permissions \u2705 manage_runners:org (Manage org runners and runner groups)","title":"REQUIREMENTS"},{"location":"vre-infra_DEPLOYMENT/#preparation","text":"CLONE REPO git clone https://github.com/vre-charite-dev/vre-infra.git cd vre-infra OPTIONAL: CLONE BY TAG # ONCE / IF THERE ARE TAGS YOU COULD SWITCH THAT WAY TO A RELEASED VERSION git checkout tags/<tag-name> OPTIONAL: CHECKOUT A SPECIFC HASH FROM COMMIT HISTORY git checkout <commit-hash>","title":"PREPARATION"},{"location":"vre-infra_DEPLOYMENT/#configuration","text":"OPTION A: EDIT EXISTING CONFIGURATION FILE # EDIT FILE / MAKE CHANGES e.g vi ./terraform/config/charite/charite.tfvars OPTION B: CREATE NEW ENVIORONMENT / CONFIGURATION FILE ENV=production # just an example name PATH_DEFAULT_CONFIG=./terraform/config/charite/charite.tfvars PATH_NEW_CONFIG=./terraform/config/${ENV}/${ENV}.tfvars # CREATE FOLDER FOR ENV mkdir -p ./terraform/config/${ENV}/ cp ${PATH_DEFAULT_CONFIG} ${PATH_NEW_CONFIG} # EDIT FILE / MAKE CHANGES e.g. vi ${PATH_NEW_CONFIG}","title":"CONFIGURATION"},{"location":"vre-infra_DEPLOYMENT/#secrets","text":"OPTION A: ADD SECRETS TO ENV export TF_VAR_ghcr_token=\"${GITHUB_USERNAME}:${GITHUB_TOKEN}\" export TF_VAR_vault_token=\"\" export TF_VAR_kubeconfig=/path/to/kube/config OPTION B: ADD NEEDED SECRETS TO A FILE (SECRETS.AUTO.TFVARS) # CREATE THE SECRETS.AUTO.TFVARS FILE IN THE TERRAFORM FOLDERS GITHUB_USERNAME=\"<CHANGE-ME>\" GITHUB_TOKEN=\"<CHANGE-ME>\" VAULT_TOKEN=\"<CHANGE-ME>\" FOLDERS=(\"pre-install\" \"install\" \"intermediary-install\" \"post-install\") for TF_FOLDER in \"${FOLDERS[@]}\"; do cat <<EOF > \"./terraform/${TF_FOLDER}/secrets.auto.tfvars\" ghcr_token = \"${GITHUB_USERNAME}:${GITHUB_TOKEN}\" vault_token = \"${VAULT_TOKEN}\" EOF done","title":"SECRETS"},{"location":"vre-infra_DEPLOYMENT/#deployment","text":"OPTION A: APPLY VRE - ALL AT ONCE PATH_CONFIG=./terraform/config/charite/charite.tfvars # or change to a newly created env config file FOLDERS=(\"pre-install\" \"install\" \"intermediary-install\" \"post-install\") VAR_FILE=${PATH_CONFIG} for TF_FOLDER in \"${FOLDERS[@]}\"; do terraform -chdir=./terraform/${TF_FOLDER} init terraform -chdir=./terraform/${TF_FOLDER} apply --auto-approve -var-file=${VAR_FILE} done OPTION B: APPLY VRE - ONE BY ONE ### 1. PRE-INSTALL pre-install creates: * cert-manager, ingress, observability, etc. * multiple secrets for authentication (regcred and vault-secret across various namespaces) * two Helm releases (cert-manager and trust-manager) Apply pre-install cd ./terraform/pre-install terraform init terraform apply --auto-approve Verify pre-install w/ kubectl+helm export KUBECONFIG=/path/to/kube/config # GENERAL kubectl get namespaces # EXAMPLE CHECKS kubectl get namespace cert-manager ingress observability kubectl get secret regcred -n cert-manager kubectl describe secret regcred -n cert-manager # GENERAL helm list -A # EXAMPLE CHECKS helm status cert-manager -n cert-manager helm status trust-manager -n cert-manager kubectl get pods -n cert-manager kubectl get pods -n ingress kubectl get pods -n observability ### 2. INTERMEDIARY-INSTALL intermediary-install creates: * cert-manager resources, including a self-signed issuer, a root certificate + cluster issuer * server certificates were generated for the operator and a MinIO tenant * jaeger operator Apply intermediary-install cd ./terraform/intermediary-install # or cd ../intermediary-install changing from previous apply operation terraform init terraform apply --auto-approve Verify intermediary-install w/ kubectl+helm # List all ClusterIssuers and Issuers kubectl get clusterissuers,issuers -A # Describe a specific ClusterIssuer kubectl describe clusterissuer <issuer-name> # List all Certificates kubectl get certificates -A # Describe a specific Certificate kubectl describe certificate <certificate-name> -n <namespace> # Check CertificateRequest status kubectl get certificaterequests -A # List Secrets to confirm certificates were created kubectl get secrets -A | grep tls # Inspect a specific TLS secret kubectl describe secret <secret-name> -n <namespace> # Check Helm releases helm list -A | grep jaeger # Get detailed status of Jaeger Operator Helm release helm status jaeger -n <namespace> # Check Jaeger Operator pods kubectl get pods -n <namespace> | grep jaeger # Describe Jaeger Operator deployment kubectl describe deployment jaeger-operator -n <namespace> # Check logs for troubleshooting kubectl logs -l app.kubernetes.io/name=jaeger-operator -n <namespace> ### 3. INSTALL install creates: * Core Services: Kong, Redis, Elasticsearch, Neo4j, OpenLDAP, MinIO. * Application Services: Approval, Encryption, Cataloguing, Dataset, Provenance, Notification, etc. * Utility Services: Mailhog, Queue Consumer, Queue Producer, Pipeline Watch, etc. Apply install cd ./terraform/install # or cd ../install changing from previous apply operation terraform init terraform apply --auto-approve -var-file=\"../config/charite/charite.tfvars\" Verify install w/ kubectl+helm # List All Helm Releases helm list --all-namespaces # Check Resources for a Specific Helm Release helm status <release-name> -n <namespace> # Example Commands for Specific Releases helm status kong -n <namespace> kubectl get pods -n <namespace> -l app.kubernetes.io/instance=kong helm status redis -n <namespace> kubectl get services -n <namespace> -l app.kubernetes.io/instance=redis kubectl get configmaps -n <namespace> -l app.kubernetes.io/instance=<release-name> kubectl get secrets -n <namespace> -l app.kubernetes.io/instance=<release-name> kubectl get pv kubectl get pvc -n <namespace> ### 4. POST-INSTALL post-install creates: * Atlas: ConfigMap and Job for Atlas configuration. * Elasticsearch and Kong: ConfigMaps and Jobs for Elasticsearch and Kong setup. * Keycloak: * A new realm (vre). * Multiple OpenID clients (react_app_client, minio_client, kong_client). * Roles (platform_admin, admin_role). * Protocol mappers for client configurations. * A user (admin_user) with assigned roles. * Service account roles and client default scopes. Apply post-install CONFIG_FILE=\"../config/charite/charite.tfvars\" # example cd ./terraform/post-install # or cd ../post-install changing from previous apply operation terraform init terraform apply --auto-approve -var-file=\"${CONFIG_FILE} Verify post-install w/ kubectl+helm # Check Atlas ConfigMap kubectl get configmap atlas-custom-entity -n utility # Check Elasticsearch ConfigMap kubectl get configmap elasticsearch-index-configmap -n <namespace> # Check Elasticsearch Job kubectl get job elasticsearch-index-job -n <namespace> # Check Kong ConfigMap kubectl get configmap kong-configmap -n <namespace> # Check Kong Job kubectl get job kong-job -n <namespace> # Check Atlas Job kubectl get job atlas-config-job -n utility # Verify Keycloak Realm (vre) kubectl exec -it <keycloak-pod> -n <namespace> -- /opt/keycloak/bin/kcadm.sh get realms/vre # Verify OpenID Clients kubectl exec -it <keycloak-pod> -n <namespace> -- /opt/keycloak/bin/kcadm.sh get clients -r vre # Verify Client Default Scopes kubectl exec -it <keycloak-pod> -n <namespace> -- /opt/keycloak/bin/kcadm.sh get clients/<client-id>/default-client-scopes -r vre","title":"DEPLOYMENT"},{"location":"vre-infra_DEPLOYMENT/#troubleshooting-terraform","text":"Check Terraform Output Terraform usually provides error messages when a run fails. Review the output: terraform apply --auto-approve To capture the output into a file for easier review: terraform apply --auto-approve | tee terraform_output.log Verify Environment Variables echo $GITHUB_USERNAME echo $GITHUB_TOKEN | sed 's/./*/g' # Masked output for security echo $VAULT_TOKEN | sed 's/./*/g' Alternatively, use a .env file and source it: source .env Validate Terraform Configuration Check for syntax or structural errors in the Terraform files: terraform validate If validation passes but apply still fails, check the execution plan: terraform plan Enable Debug Logging For more details on the error, enable debug logging: export TF_LOG=DEBUG terraform apply --auto-approve 2>&1 | tee terraform_debug.log Restart Terraform from Scratch If troubleshooting doesn\u2019t resolve the issue, reset Terraform and try again: terraform destroy --auto-approve # Destroy existing resources rm -rf .terraform/ terraform.tfstate* # Delete local state terraform init terraform apply --auto-approve","title":"TROUBLESHOOTING TERRAFORM"},{"location":"vre-infra_DEPLOYMENT/#uninstall","text":"OPTION A: DESTROY VRE - ALL AT ONCE PATH_CONFIG=$(pwd)/terraform/config/charite/charite.tfvars # or change to a newly created env config file FOLDERS=(\"post-install\" \"install\" \"intermediary-install\" \"pre-install\") VAR_FILE=${PATH_CONFIG} for TF_FOLDER in \"${FOLDERS[@]}\"; do echo destroying ${TF_FOLDER} terraform -chdir=./terraform/${TF_FOLDER} destroy --auto-approve -var-file=${VAR_FILE} echo destroy ${TF_FOLDER} complete! done OPTION B: DESTROY SPECIFIC VRE DEPLOYMENT STEP CONFIG_FILE=\"../config/charite/charite.tfvars\" # example, needed for at least install & post-install steps cd ./terraform/<INSTALL-NAME> # e.g. install or post-install terraform destroy --auto-approve -var-file=\"${CONFIG_FILE}","title":"UNINSTALL"}]}